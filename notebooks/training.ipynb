{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e035c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from itertools import pairwise, accumulate\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.nn.functional import max_pool2d, interpolate\n",
    "\n",
    "from config import CHANNELS_DIMENSION\n",
    "# from datasets import PreprocessedOpenFWI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edaf380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ConvBlock(nn.Sequential):\n",
    "    \"\"\"3x3+1Padding Conv, BN, ReLu\"\"\"\n",
    "    def __init__(self, in_channels:int, out_channels:int):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"2 Convulution block + residual\"\"\"\n",
    "    def __init__(self, in_channels:int, out_channels:int):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.Sequential(\n",
    "            ConvBlock(in_channels, out_channels),\n",
    "            ConvBlock(out_channels, out_channels),\n",
    "        )\n",
    "        if in_channels == out_channels:\n",
    "            self.skip_connection = nn.Identity() \n",
    "        else:\n",
    "            # May want to set bias to False ?\n",
    "            self.skip_connection = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        return self.skip_connection(x) + self.blocks(x)\n",
    "\n",
    "def encode(x:Tensor, module:nn.Module) -> Tensor:\n",
    "    return max_pool2d(module(x), 2)\n",
    "\n",
    "def decode(prev_block_x:Tensor, skip_x:Tensor, module:nn.Module) -> Tensor:\n",
    "    x_diff = skip_x.shape[2] - prev_block_x.shape[2]\n",
    "    y_diff = skip_x.shape[3] - prev_block_x.shape[3]\n",
    "    print(\"prev_block_x.shape:\", prev_block_x.shape, \"skip_x.shape:\", skip_x.shape)\n",
    "    # todo: Center padding?\n",
    "    print(\"x_diff:\", x_diff, \"y_diff:\", y_diff, \"pad:\", (x_diff, 0, y_diff, 0))\n",
    "    padded_prev_block_x = nn.functional.pad(prev_block_x, (x_diff, 0, y_diff, 0))\n",
    "    print(\"padded_prev_block_x.shape:\", padded_prev_block_x.shape)\n",
    "    \n",
    "    x = torch.concatenate((padded_prev_block_x, skip_x), CHANNELS_DIMENSION)\n",
    "    print(\"concat x.shape:\", x.shape)\n",
    "    out = module(x)\n",
    "    return interpolate(out, scale_factor=2, mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels:int, out_channels:int, start_features:int, depth:int):\n",
    "        super().__init__()\n",
    "        # Define the channels per depth of the Unet\n",
    "        chs_per_depth = [start_features * 2 ** i for i in range(depth)]\n",
    "        # Instantiate the Unet\n",
    "        down_blocks_chns_it = pairwise([in_channels, *chs_per_depth])\n",
    "        self.down_blocks = [ResidualBlock(in_chs, out_chs) for in_chs, out_chs in down_blocks_chns_it]\n",
    "        self.down_blocks = nn.ModuleList(self.down_blocks)\n",
    "        # Instantiate the bottle neck\n",
    "        self.bottle_neck_block = ResidualBlock(chs_per_depth[-1], chs_per_depth[-1])\n",
    "        # Instantialte the downblocks\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        for in_chs, out_chs in pairwise([*chs_per_depth[::-1], out_channels]):\n",
    "            self.up_blocks.append(ResidualBlock(in_chs * 2, out_chs))\n",
    "        # up_blocks_chans_it = pairwise([*chs_per_depth[::-1], out_channels])\n",
    "        # self.up_blocks = [ResidualBlock(in_chs, out_chs) for in_chs, out_chs in up_blocks_chans_it]\n",
    "        print(len(self.up_blocks))\n",
    "\n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        encoder_outputs = list(accumulate(self.down_blocks, encode, initial=x))\n",
    "        for i, encoder_output in enumerate(encoder_outputs):\n",
    "            print(i, encoder_output.shape)\n",
    "        bottleneck_output = self.bottle_neck_block(encoder_outputs[-1])\n",
    "        out = bottleneck_output\n",
    "        print(\"up:\", len(self.up_blocks))\n",
    "        print(\"encoder_outputs:\", len(encoder_outputs[::-1]))\n",
    "        for up_block, encode_output in zip(self.up_blocks, encoder_outputs[::-1]):\n",
    "            print(\"=======\")\n",
    "            print(\"out:\", out.shape)\n",
    "            print(\"encode_output:\", encode_output.shape)\n",
    "            out = decode(out, encode_output, up_block)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = UNet(5, 1, 32, 4)#.cuda()\n",
    "\n",
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "15894089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([200, 5, 72, 72])\n",
      "1 torch.Size([200, 32, 36, 36])\n",
      "2 torch.Size([200, 64, 18, 18])\n",
      "3 torch.Size([200, 128, 9, 9])\n",
      "4 torch.Size([200, 256, 4, 4])\n",
      "up: 4\n",
      "encoder_outputs: 5\n",
      "=======\n",
      "out: torch.Size([200, 256, 4, 4])\n",
      "encode_output: torch.Size([200, 256, 4, 4])\n",
      "prev_block_x.shape: torch.Size([200, 256, 4, 4]) skip_x.shape: torch.Size([200, 256, 4, 4])\n",
      "x_diff: 0 y_diff: 0 pad: (0, 0, 0, 0)\n",
      "padded_prev_block_x.shape: torch.Size([200, 256, 4, 4])\n",
      "concat x.shape: torch.Size([200, 512, 4, 4])\n",
      "=======\n",
      "out: torch.Size([200, 128, 8, 8])\n",
      "encode_output: torch.Size([200, 128, 9, 9])\n",
      "prev_block_x.shape: torch.Size([200, 128, 8, 8]) skip_x.shape: torch.Size([200, 128, 9, 9])\n",
      "x_diff: 1 y_diff: 1 pad: (1, 0, 1, 0)\n",
      "padded_prev_block_x.shape: torch.Size([200, 128, 9, 9])\n",
      "concat x.shape: torch.Size([200, 256, 9, 9])\n",
      "=======\n",
      "out: torch.Size([200, 64, 18, 18])\n",
      "encode_output: torch.Size([200, 64, 18, 18])\n",
      "prev_block_x.shape: torch.Size([200, 64, 18, 18]) skip_x.shape: torch.Size([200, 64, 18, 18])\n",
      "x_diff: 0 y_diff: 0 pad: (0, 0, 0, 0)\n",
      "padded_prev_block_x.shape: torch.Size([200, 64, 18, 18])\n",
      "concat x.shape: torch.Size([200, 128, 18, 18])\n",
      "=======\n",
      "out: torch.Size([200, 32, 36, 36])\n",
      "encode_output: torch.Size([200, 32, 36, 36])\n",
      "prev_block_x.shape: torch.Size([200, 32, 36, 36]) skip_x.shape: torch.Size([200, 32, 36, 36])\n",
      "x_diff: 0 y_diff: 0 pad: (0, 0, 0, 0)\n",
      "padded_prev_block_x.shape: torch.Size([200, 32, 36, 36])\n",
      "concat x.shape: torch.Size([200, 64, 36, 36])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 1, 72, 72])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input = torch.randn(200, 5, 72, 72)#.cuda()\n",
    "\n",
    "model(test_input).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GEO-FWI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
